{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLJWpyOIvBOkmYXC/NbVP4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "HISqlyClQFlX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GYukVc9nQ-dz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL VARIABLES\n",
        "ensemble_size = 3\n",
        "batch_size = 64\n",
        "num_epochs = 40\n",
        "learning_rate = 0.001\n",
        "\n",
        "# ARRAY TO DICTIONARY RESULTS\n",
        "results = []"
      ],
      "metadata": {
        "id": "OjfoS4oTRVQb"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "b3-FR6QTRBpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for training and testing data\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "id": "5j-tvEOuREtp"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])"
      ],
      "metadata": {
        "id": "NKDiteZrSa-q"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset: Training & Test Data\n",
        "train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "OGOEtCRiRF1k",
        "outputId": "5ffeb9ce-dcbf-42dc-a731-aa033f304f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the base length for each subset\n",
        "subset_length = len(train) // 3\n",
        "\n",
        "# Calculate the remainder\n",
        "remainder = len(train) % 3\n",
        "\n",
        "# Distribute the remainder among the subsets\n",
        "lengths = [subset_length + (1 if i < remainder else 0) for i in range(3)]"
      ],
      "metadata": {
        "id": "oEM4Nw9KSy_w"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_length, remainder, lengths"
      ],
      "metadata": {
        "id": "cYlHBVlMS1Wk",
        "outputId": "c1b6de98-35d8-486b-d25b-52ad5cf2c2f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16666, 2, [16667, 16667, 16666])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training dataset into three subsets for the ensemble\n",
        "train_subset_1, train_subset_2, train_subset_3 = random_split(train, lengths)\n",
        "\n",
        "# Individual trainloaders for specific subsets\n",
        "trainloader_1 = DataLoader(train_subset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "trainloader_2 = DataLoader(train_subset_2, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "trainloader_3 = DataLoader(train_subset_3, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# test definition\n",
        "testloader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "cbO78dKFRR_n"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Architecture (Simple CNN)"
      ],
      "metadata": {
        "id": "LKST0PkETK96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Adding batch normalization layers after each convolutional layer\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Adding dropout with a probability of 0.5\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply batch norm and dropout after ReLU activations\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout only on fully connected layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1WWoD_6sRr_1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainloader, criterion, optimizer):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"\\nTraining Finisehd, time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Clear the CUDA memory cache after training each model\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JgxZZ8ora3nV"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, model_name=\"\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in testloader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n",
        "    return dict(name=model_name, accuracy=(100 * correct / total), epochs=num_epochs, learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "FtTtGU3te3gC"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: CNN with first subset of training *data*\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()` initializes cross-entropy loss as the criterion, a commonly used loss function for multi-class classification tasks. Cross-entropy loss measures the difference between the modelâ€™s predicted class probabilities and the actual class labels, with a higher penalty for incorrect predictions. This guides the model toward learning accurate class probabilities by minimizing this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=learning_rate)` sets up the Adam optimizer, which will adjust the modelâ€™s parameters at each training step. Here, `model.parameters()` specifies the parameters to be updated, and `lr=learning_rate` defines the learning rate, controlling the size of the updates applied to the modelâ€™s parameters. The optimizer and loss function work together to enable effective backpropagation and parameter adjustment, essential for improving the model's performance as training progresses."
      ],
      "metadata": {
        "id": "sT6rXMX9UJWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"base_learner_1\"\n",
        "base_learner_1 = SimpleCNN().to(device) # Enable model to run on CUDA\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(base_learner_1.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "4XnbLKoaUStS"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_state_dict = torch.load('base_learner_1.pth')"
      ],
      "metadata": {
        "id": "sgbHqqzXn1j2",
        "outputId": "877c8b38-319e-45c5-ba53-107ed6edc80b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-86-3ff9608cba92>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_state_dict = torch.load('base_learner_1.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if loaded_state_dict.keys() == base_learner_1.state_dict().keys():\n",
        "    print(\"Matching model architecture found. Loading weights...\")\n",
        "    base_learner_1.load_state_dict(loaded_state_dict)\n",
        "else:\n",
        "    print(\"Model architecture does not match. Retraining the model...\")\n",
        "    # Retrain your model (assuming you have a function `train_model`)\n",
        "    train_model(base_learner_1, trainloader_1, criterion, optimizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5dKzDBA2cvHN",
        "outputId": "466b1aa8-5904-4b68-f1ea-9d7e0999b28a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching model architecture found. Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(base_learner_1.state_dict(), 'base_learner_1.pth')"
      ],
      "metadata": {
        "id": "TwWzkAalm0Kf"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation for base learner (1)"
      ],
      "metadata": {
        "id": "ZNt_94KZU4TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(base_learner_1, testloader, model_name=\"base_learner_1\")"
      ],
      "metadata": {
        "id": "g8gzNKB5fRkW",
        "outputId": "76cc2312-ce82-4534-b781-a05a3ec2cd96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 72.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "rM35nuYNfgS5",
        "outputId": "8cde1772-7268-4ba8-ea20-0fb2c6e7956c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'base_learner_1',\n",
              " 'accuracy': 72.85,\n",
              " 'epochs': 40,\n",
              " 'learning_rate': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "CkhAl-rlfe3A"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: CNN with second subset of training *data*\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()` initializes cross-entropy loss as the criterion, a commonly used loss function for multi-class classification tasks. Cross-entropy loss measures the difference between the modelâ€™s predicted class probabilities and the actual class labels, with a higher penalty for incorrect predictions. This guides the model toward learning accurate class probabilities by minimizing this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=learning_rate)` sets up the Adam optimizer, which will adjust the modelâ€™s parameters at each training step. Here, `model.parameters()` specifies the parameters to be updated, and `lr=learning_rate` defines the learning rate, controlling the size of the updates applied to the modelâ€™s parameters. The optimizer and loss function work together to enable effective backpropagation and parameter adjustment, essential for improving the model's performance as training progresses."
      ],
      "metadata": {
        "id": "rgvliFpXVp7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learner_2 = SimpleCNN().to(device) # Enable model to run on CUDA\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(base_learner_2.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "wVtZSsWFVo0y"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_state_dict = torch.load('base_learner_2.pth', weights_only=True)"
      ],
      "metadata": {
        "id": "snUvmFiToL9E"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if loaded_state_dict.keys() == base_learner_2.state_dict().keys():\n",
        "    print(\"Matching model architecture found. Loading weights...\")\n",
        "    base_learner_2.load_state_dict(loaded_state_dict)\n",
        "else:\n",
        "    print(\"Model architecture does not match. Retraining the model...\")\n",
        "    # Retrain your model (assuming you have a function `train_model`)\n",
        "    train_model(base_learner_2, trainloader_2, criterion, optimizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B3dsGuqUdIYC",
        "outputId": "107ed77e-58e9-4e63-c6b1-c504f0641772",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching model architecture found. Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(base_learner_2.state_dict(), 'base_learner_2.pth')"
      ],
      "metadata": {
        "id": "dQmRGUztoVSm"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation for base learner (2)\n"
      ],
      "metadata": {
        "id": "VCh5lRbAfopn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(base_learner_2, testloader, model_name=\"learner_2\")"
      ],
      "metadata": {
        "id": "mVTnPrvWft4F",
        "outputId": "37683af6-467b-41a4-ef29-3f1edf595c5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 73.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "ROY54_20fv4b",
        "outputId": "a77c4b3c-4241-493f-edb0-498357c0c7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'learner_2', 'accuracy': 73.48, 'epochs': 40, 'learning_rate': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "5Q_jmNtzfw9U"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: CNN with third subset of training *data*\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()` initializes cross-entropy loss as the criterion, a commonly used loss function for multi-class classification tasks. Cross-entropy loss measures the difference between the modelâ€™s predicted class probabilities and the actual class labels, with a higher penalty for incorrect predictions. This guides the model toward learning accurate class probabilities by minimizing this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=learning_rate)` sets up the Adam optimizer, which will adjust the modelâ€™s parameters at each training step. Here, `model.parameters()` specifies the parameters to be updated, and `lr=learning_rate` defines the learning rate, controlling the size of the updates applied to the modelâ€™s parameters. The optimizer and loss function work together to enable effective backpropagation and parameter adjustment, essential for improving the model's performance as training progresses."
      ],
      "metadata": {
        "id": "A55H1ropY8ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learner_3 = SimpleCNN().to(device) # Enable model to run on CUDA\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(base_learner_3.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "fSiDCPa6ZTsj"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_state_dict = torch.load('base_learner_3.pth', weights_only=True)"
      ],
      "metadata": {
        "id": "7omSG5lCoxWD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if loaded_state_dict.keys() == base_learner_3.state_dict().keys():\n",
        "    print(\"Matching model architecture found. Loading weights...\")\n",
        "    base_learner_3.load_state_dict(loaded_state_dict)\n",
        "else:\n",
        "    print(\"Model architecture does not match. Retraining the model...\")\n",
        "    # Retrain your model (assuming you have a function `train_model`)\n",
        "    train_model(base_learner_3, trainloader_3, criterion, optimizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZbKodtHaf0gC",
        "outputId": "3574f127-5f06-43e1-ff65-e6177d2f590f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching model architecture found. Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(base_learner_3.state_dict(), 'base_learner_3.pth')"
      ],
      "metadata": {
        "id": "PxkrRc3_oYpI"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation for base learner (3)"
      ],
      "metadata": {
        "id": "8i41oGkXgI_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(base_learner_3, testloader, model_name=\"learner_3\")"
      ],
      "metadata": {
        "id": "4WWPF6ePf3D8",
        "outputId": "d6ddb359-8500-41db-99d8-188e5664610d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 72.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "F8BHGwlBf23D",
        "outputId": "e9ad007c-1108-4353-b4fe-5a68e959596a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'learner_3', 'accuracy': 72.53, 'epochs': 40, 'learning_rate': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "ECkeAzz5f5ec"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results before ensembling"
      ],
      "metadata": {
        "id": "4HRbP9b9iiTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "uEhNfVZMinNX",
        "outputId": "160742e9-a12e-4d97-8033-3b250668abcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'base_learner_1',\n",
              "  'accuracy': 72.85,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_2',\n",
              "  'accuracy': 73.48,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_3',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001}]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Predictions\n"
      ],
      "metadata": {
        "id": "fsimInp_lTwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ensemble(models, testloader, model_names=None):\n",
        "    print(\"Evaluating ensemble...\")\n",
        "\n",
        "    # Set models to evaluation mode\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "\n",
        "            # Get predictions from all models\n",
        "            model_preds = []\n",
        "            for model in models:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                model_preds.append(predicted.cpu().numpy())\n",
        "\n",
        "            # Convert model_preds to a numpy array (models x samples)\n",
        "            model_preds = np.array(model_preds)\n",
        "\n",
        "            # Majority voting (take the class with the most votes for each sample)\n",
        "            final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=model_preds)\n",
        "\n",
        "            all_preds.extend(final_preds)\n",
        "            total += labels.size(0)\n",
        "            correct += (final_preds == labels.cpu().numpy()).sum()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
        "    return dict(name=f\"ensemble:[{', '.join(model_names)}]\", accuracy=accuracy, epochs=num_epochs, learning_rate=learning_rate)\n"
      ],
      "metadata": {
        "id": "jvSTsbEllx51"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [base_learner_1, base_learner_2, base_learner_3]\n",
        "model_names = [\"BL1\", \"BL2\", \"BL3\"]"
      ],
      "metadata": {
        "id": "pfPMW0S3l3S4"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_ensemble(models, testloader, model_names)"
      ],
      "metadata": {
        "id": "eWCp5SBfmbzH",
        "outputId": "1a8eccad-b261-41d9-83b6-ba27319e9cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ensemble...\n",
            "Accuracy on test set: 76.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "Vq2P0AoYmLae"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "H05zizeCp4BP",
        "outputId": "4d4b5cf4-8cfc-4d58-937b-11b85958e336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'base_learner_1',\n",
              "  'accuracy': 72.85,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_2',\n",
              "  'accuracy': 73.48,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_3',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'ensemble:[BL1, BL2, BL3]',\n",
              "  'accuracy': 76.6,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001}]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    }
  ]
}