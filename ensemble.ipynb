{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfN1K2lsz65DXYCYqDDMBH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HISqlyClQFlX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import save_file\n",
        "from safetensors.torch import load_file"
      ],
      "metadata": {
        "id": "g5lElR65s3ks"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GYukVc9nQ-dz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL VARIABLES\n",
        "ensemble_size = 3\n",
        "batch_size = 64\n",
        "num_epochs = 40\n",
        "learning_rate = 0.001\n",
        "\n",
        "LOAD_MODELS = False\n",
        "\n",
        "# ARRAY TO DICTIONARY RESULTS\n",
        "results = []"
      ],
      "metadata": {
        "id": "OjfoS4oTRVQb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "b3-FR6QTRBpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for training and testing data\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "id": "5j-tvEOuREtp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])"
      ],
      "metadata": {
        "id": "NKDiteZrSa-q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset: Training & Test Data\n",
        "train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGOEtCRiRF1k",
        "outputId": "b680334a-d52f-4c75-9a0b-a605d99e1383"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the base length for each subset\n",
        "subset_length = len(train) // 3\n",
        "\n",
        "# Calculate the remainder\n",
        "remainder = len(train) % 3\n",
        "\n",
        "# Distribute the remainder among the subsets\n",
        "lengths = [subset_length + (1 if i < remainder else 0) for i in range(3)]"
      ],
      "metadata": {
        "id": "oEM4Nw9KSy_w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_length, remainder, lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYlHBVlMS1Wk",
        "outputId": "e9c9e26e-ed8d-4ecd-81fb-2fb4f415da51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16666, 2, [16667, 16667, 16666])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training dataset into three subsets for the ensemble\n",
        "train_subset_1, train_subset_2, train_subset_3 = random_split(train, lengths)\n",
        "\n",
        "# Individual trainloaders for specific subsets\n",
        "trainloader_1 = DataLoader(train_subset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "trainloader_2 = DataLoader(train_subset_2, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "trainloader_3 = DataLoader(train_subset_3, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# test definition\n",
        "testloader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "cbO78dKFRR_n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Architecture (Simple CNN)"
      ],
      "metadata": {
        "id": "LKST0PkETK96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Adding batch normalization layers after each convolutional layer\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Adding dropout with a probability of 0.5\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply batch norm and dropout after ReLU activations\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout only on fully connected layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "1WWoD_6sRr_1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainloader, criterion, optimizer):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"\\nTraining Finisehd, time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Clear the CUDA memory cache after training each model\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JgxZZ8ora3nV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, model_name=\"\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in testloader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n",
        "    return dict(name=model_name, accuracy=(100 * correct / total), epochs=num_epochs, learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "FtTtGU3te3gC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: CNN with first subset of training *data*\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()` initializes cross-entropy loss as the criterion, a commonly used loss function for multi-class classification tasks. Cross-entropy loss measures the difference between the model’s predicted class probabilities and the actual class labels, with a higher penalty for incorrect predictions. This guides the model toward learning accurate class probabilities by minimizing this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=learning_rate)` sets up the Adam optimizer, which will adjust the model’s parameters at each training step. Here, `model.parameters()` specifies the parameters to be updated, and `lr=learning_rate` defines the learning rate, controlling the size of the updates applied to the model’s parameters. The optimizer and loss function work together to enable effective backpropagation and parameter adjustment, essential for improving the model's performance as training progresses."
      ],
      "metadata": {
        "id": "sT6rXMX9UJWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"base_learner_1\"\n",
        "base_learner_1 = SimpleCNN().to(device) # Enable model to run on CUDA\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(base_learner_1.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "4XnbLKoaUStS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_state_dict = load_file('base_learner_1.safetensors')"
      ],
      "metadata": {
        "id": "sgbHqqzXn1j2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if loaded_state_dict.keys() == base_learner_1.state_dict().keys() and LOAD_MODELS:\n",
        "    print(\"Matching model architecture found. Loading weights...\")\n",
        "    base_learner_1.load_state_dict(loaded_state_dict)\n",
        "else:\n",
        "    print(\"Model architecture does not match. Retraining the model...\")\n",
        "    # Retrain your model (assuming you have a function `train_model`)\n",
        "    train_model(base_learner_1, trainloader_1, criterion, optimizer)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dKzDBA2cvHN",
        "outputId": "ccdd0ab7-95c1-4a74-c701-9b606757947c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching model architecture found. Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(base_learner_1.state_dict(), 'base_learner_1.safetensors')"
      ],
      "metadata": {
        "id": "TwWzkAalm0Kf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation for base learner (1)"
      ],
      "metadata": {
        "id": "ZNt_94KZU4TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(base_learner_1, testloader, model_name=\"base_learner_1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8gzNKB5fRkW",
        "outputId": "3c3c54f6-a858-4973-d647-27e43c403d75"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 72.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM35nuYNfgS5",
        "outputId": "bb5eb3c3-2618-4c13-d699-b85676131097"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'base_learner_1',\n",
              " 'accuracy': 72.53,\n",
              " 'epochs': 40,\n",
              " 'learning_rate': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "CkhAl-rlfe3A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: CNN with second subset of training *data*\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()` initializes cross-entropy loss as the criterion, a commonly used loss function for multi-class classification tasks. Cross-entropy loss measures the difference between the model’s predicted class probabilities and the actual class labels, with a higher penalty for incorrect predictions. This guides the model toward learning accurate class probabilities by minimizing this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=learning_rate)` sets up the Adam optimizer, which will adjust the model’s parameters at each training step. Here, `model.parameters()` specifies the parameters to be updated, and `lr=learning_rate` defines the learning rate, controlling the size of the updates applied to the model’s parameters. The optimizer and loss function work together to enable effective backpropagation and parameter adjustment, essential for improving the model's performance as training progresses."
      ],
      "metadata": {
        "id": "rgvliFpXVp7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learner_2 = SimpleCNN().to(device) # Enable model to run on CUDA\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(base_learner_2.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "wVtZSsWFVo0y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_state_dict = load_file('base_learner_2.safetensors')"
      ],
      "metadata": {
        "id": "snUvmFiToL9E"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if  loaded_state_dict.keys() == base_learner_2.state_dict().keys() and LOAD_MODELS:\n",
        "    print(\"Matching model architecture found. Loading weights...\")\n",
        "    base_learner_2.load_state_dict(loaded_state_dict)\n",
        "else:\n",
        "    print(\"Model architecture does not match. Retraining the model...\")\n",
        "    train_model(base_learner_2, trainloader_2, criterion, optimizer)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3dsGuqUdIYC",
        "outputId": "6f76f9e1-6bf0-4a87-c19a-1fd6c91b97e0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching model architecture found. Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(base_learner_2.state_dict(), 'base_learner_2.safetensors')"
      ],
      "metadata": {
        "id": "dQmRGUztoVSm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation for base learner (2)\n"
      ],
      "metadata": {
        "id": "VCh5lRbAfopn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(base_learner_2, testloader, model_name=\"learner_2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVTnPrvWft4F",
        "outputId": "b949acc1-ae75-4ca4-9878-3240c3ef7b1c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 72.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROY54_20fv4b",
        "outputId": "7a7e37e2-6109-4f42-804b-f07c8f93cda9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'learner_2', 'accuracy': 72.53, 'epochs': 40, 'learning_rate': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "5Q_jmNtzfw9U"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: CNN with third subset of training *data*\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss()` initializes cross-entropy loss as the criterion, a commonly used loss function for multi-class classification tasks. Cross-entropy loss measures the difference between the model’s predicted class probabilities and the actual class labels, with a higher penalty for incorrect predictions. This guides the model toward learning accurate class probabilities by minimizing this loss during training.\n",
        "\n",
        "`optimizer = optim.Adam(model.parameters(), lr=learning_rate)` sets up the Adam optimizer, which will adjust the model’s parameters at each training step. Here, `model.parameters()` specifies the parameters to be updated, and `lr=learning_rate` defines the learning rate, controlling the size of the updates applied to the model’s parameters. The optimizer and loss function work together to enable effective backpropagation and parameter adjustment, essential for improving the model's performance as training progresses."
      ],
      "metadata": {
        "id": "A55H1ropY8ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learner_3 = SimpleCNN().to(device) # Enable model to run on CUDA\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(base_learner_3.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "fSiDCPa6ZTsj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_state_dict = load_file('base_learner_3.safetensors')"
      ],
      "metadata": {
        "id": "7omSG5lCoxWD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if loaded_state_dict.keys() == base_learner_3.state_dict().keys() and LOAD_MODELS:\n",
        "    print(\"Matching model architecture found. Loading weights...\")\n",
        "    base_learner_3.load_state_dict(loaded_state_dict)\n",
        "else:\n",
        "    print(\"Model architecture does not match. Retraining the model...\")\n",
        "    # Retrain your model (assuming you have a function `train_model`)\n",
        "    train_model(base_learner_3, trainloader_3, criterion, optimizer)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbKodtHaf0gC",
        "outputId": "fb1600b5-3a08-44d8-d201-038082a3ad16"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching model architecture found. Loading weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(base_learner_3.state_dict(), 'base_learner_3.safetensors')"
      ],
      "metadata": {
        "id": "PxkrRc3_oYpI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation for base learner (3)"
      ],
      "metadata": {
        "id": "8i41oGkXgI_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_model(base_learner_3, testloader, model_name=\"learner_3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WWPF6ePf3D8",
        "outputId": "07f1607a-e817-49f4-8cb6-d781f6ab5b59"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 72.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8BHGwlBf23D",
        "outputId": "a4a484d9-824d-486a-bca6-46155799d5af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'learner_3', 'accuracy': 72.53, 'epochs': 40, 'learning_rate': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "ECkeAzz5f5ec"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results before ensembling"
      ],
      "metadata": {
        "id": "4HRbP9b9iiTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEhNfVZMinNX",
        "outputId": "a567ee75-8d61-437c-83dd-4c31f8cdf79e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'base_learner_1',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_2',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_3',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Predictions\n"
      ],
      "metadata": {
        "id": "fsimInp_lTwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ensemble(models, testloader, model_names=None):\n",
        "    print(\"Evaluating ensemble...\")\n",
        "\n",
        "    # Set models to evaluation mode\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "\n",
        "            # Get predictions from all models\n",
        "            model_preds = []\n",
        "            for model in models:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                model_preds.append(predicted.cpu().numpy())\n",
        "\n",
        "            # Convert model_preds to a numpy array (models x samples)\n",
        "            model_preds = np.array(model_preds)\n",
        "\n",
        "            # Majority voting (take the class with the most votes for each sample)\n",
        "            final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=model_preds)\n",
        "\n",
        "            all_preds.extend(final_preds)\n",
        "            total += labels.size(0)\n",
        "            correct += (final_preds == labels.cpu().numpy()).sum()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
        "    return dict(name=f\"ensemble:[{', '.join(model_names)}]\", accuracy=accuracy, epochs=num_epochs, learning_rate=learning_rate)\n"
      ],
      "metadata": {
        "id": "jvSTsbEllx51"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [base_learner_1, base_learner_2, base_learner_3]\n",
        "model_names = [\"BL1\", \"BL2\", \"BL3\"]"
      ],
      "metadata": {
        "id": "pfPMW0S3l3S4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate_ensemble(models, testloader, model_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWCp5SBfmbzH",
        "outputId": "1c4f64ae-1019-4931-f6ed-108b1a94ed1d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ensemble...\n",
            "Accuracy on test set: 72.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.append(result)"
      ],
      "metadata": {
        "id": "Vq2P0AoYmLae"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H05zizeCp4BP",
        "outputId": "ae348e31-6b7d-4eab-f543-d586249c1c02"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'base_learner_1',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_2',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'learner_3',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001},\n",
              " {'name': 'ensemble:[BL1, BL2, BL3]',\n",
              "  'accuracy': 72.53,\n",
              "  'epochs': 40,\n",
              "  'learning_rate': 0.001}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}